{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuel Blend Properties Prediction: Exploratory Data Analysis\n",
    "\n",
    "This notebook explores the fuel blend dataset to understand its structure, relationships, and prepare for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Basic Data Examination\n",
    "\n",
    "Let's start by loading the datasets and examining their basic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_data = pd.read_csv('dataset/train.csv')\n",
    "test_data = pd.read_csv('dataset/test.csv')\n",
    "sample_solution = pd.read_csv('dataset/sample_solution.csv')\n",
    "\n",
    "print(\"\\n=== DATASET DIMENSIONS ===\")\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "print(f\"Sample solution shape: {sample_solution.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the first few rows of each dataset\n",
    "print(\"First 5 rows of training data:\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 5 rows of test data:\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 5 rows of sample solution:\")\n",
    "sample_solution.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify column groups\n",
    "fraction_cols = [col for col in train_data.columns if 'fraction' in col]\n",
    "property_cols = [col for col in train_data.columns if 'Property' in col and 'Blend' not in col]\n",
    "target_cols = [col for col in train_data.columns if 'BlendProperty' in col]\n",
    "\n",
    "print(f\"Component fraction columns ({len(fraction_cols)}): {fraction_cols}\")\n",
    "print(f\"\\nComponent property columns ({len(property_cols)}): First 10 shown\")\n",
    "print(property_cols[:10])\n",
    "print(f\"\\nTarget columns ({len(target_cols)}): {target_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\n=== MISSING VALUES CHECK ===\")\n",
    "print(f\"Missing values in training data: {train_data.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in test data: {test_data.isnull().sum().sum()}\")\n",
    "\n",
    "# Check data types\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(\"Training data types:\")\n",
    "print(train_data.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component Fractions Verification\n",
    "\n",
    "Let's verify that the component fractions sum to 1.0 for each blend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if fractions sum to 1\n",
    "fraction_sums = train_data[fraction_cols].sum(axis=1)\n",
    "print(f\"Fraction sums - Min: {fraction_sums.min():.6f}, Max: {fraction_sums.max():.6f}\")\n",
    "print(f\"All fractions sum to 1.0: {np.allclose(fraction_sums, 1.0, atol=1e-10)}\")\n",
    "\n",
    "# Visualize the distribution of fraction sums\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(fraction_sums, bins=50)\n",
    "plt.axvline(x=1.0, color='r', linestyle='--', label='Expected Sum (1.0)')\n",
    "plt.title('Distribution of Component Fraction Sums')\n",
    "plt.xlabel('Sum of Component Fractions')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for component fractions\n",
    "print(\"Component fractions summary:\")\n",
    "train_data[fraction_cols].describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for target properties\n",
    "print(\"Target properties summary:\")\n",
    "train_data[target_cols].describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA QUALITY CHECKS ===\")\n",
    "print(f\"Duplicate rows in training data: {train_data.duplicated().sum()}\")\n",
    "print(f\"Duplicate rows in test data: {test_data.duplicated().sum()}\")\n",
    "print(f\"Infinite values in training data: {np.isinf(train_data.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "print(f\"Infinite values in test data: {np.isinf(test_data.select_dtypes(include=[np.number])).sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component Usage Analysis\n",
    "\n",
    "Let's analyze how often each component is used in the blends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for zero fractions (components not used)\n",
    "zero_fractions = {}\n",
    "for col in fraction_cols:\n",
    "    zero_count = (train_data[col] == 0).sum()\n",
    "    zero_pct = (zero_count / len(train_data)) * 100\n",
    "    zero_fractions[col] = zero_pct\n",
    "    \n",
    "# Plot component usage\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(zero_fractions.keys(), [100 - v for v in zero_fractions.values()])\n",
    "plt.title('Component Usage in Blends')\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Percentage of Blends Using Component')\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "             f'{height:.1f}%', ha='center', va='bottom')\n",
    "plt.show()\n",
    "\n",
    "# Print the actual percentages\n",
    "for col, pct in zero_fractions.items():\n",
    "    zero_count = (train_data[col] == 0).sum()\n",
    "    print(f\"{col}: {zero_count} zeros ({pct:.1f}% not used, {100-pct:.1f}% used)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Component Fractions Analysis\n",
    "\n",
    "Let's analyze the distribution and relationships of component fractions in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of component fractions\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(fraction_cols, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.histplot(train_data[col], kde=True, bins=30)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel('Fraction Value')\n",
    "    plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots of component fractions\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=train_data[fraction_cols])\n",
    "plt.title('Component Fractions Box Plot')\n",
    "plt.ylabel('Fraction Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between component fractions\n",
    "plt.figure(figsize=(10, 8))\n",
    "fraction_corr = train_data[fraction_cols].corr()\n",
    "sns.heatmap(fraction_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
    "plt.title('Correlation Between Component Fractions')\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation matrix of component fractions:\")\n",
    "fraction_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot of component fractions\n",
    "sns.pairplot(train_data[fraction_cols], diag_kind='kde', height=2)\n",
    "plt.suptitle('Pairwise Relationships Between Component Fractions', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Blend Patterns\n",
    "\n",
    "Let's identify common blending patterns by analyzing the most frequent combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary indicators for component presence\n",
    "component_presence = train_data[fraction_cols].applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Count frequency of each combination pattern\n",
    "pattern_counts = component_presence.value_counts().reset_index()\n",
    "pattern_counts.columns = list(fraction_cols) + ['Count']\n",
    "pattern_counts['Percentage'] = pattern_counts['Count'] / len(train_data) * 100\n",
    "\n",
    "# Display the most common patterns\n",
    "print(\"Most common component combinations:\")\n",
    "pattern_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the most common patterns\n",
    "top_patterns = pattern_counts.head(8)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "for i, row in enumerate(top_patterns.itertuples(), 1):\n",
    "    plt.subplot(2, 4, i)\n",
    "    pattern = row[1:6]  # Extract the pattern (first 5 columns)\n",
    "    plt.bar(fraction_cols, pattern, color=['blue' if x > 0 else 'lightgray' for x in pattern])\n",
    "    plt.title(f'Pattern {i}: {row.Percentage:.1f}%')\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component Dominance Analysis\n",
    "\n",
    "Let's analyze which component tends to be the dominant one in blends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the dominant component in each blend\n",
    "train_data['dominant_component'] = train_data[fraction_cols].idxmax(axis=1)\n",
    "\n",
    "# Count occurrences of each dominant component\n",
    "dominant_counts = train_data['dominant_component'].value_counts()\n",
    "dominant_pct = dominant_counts / len(train_data) * 100\n",
    "\n",
    "# Plot dominant component distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(dominant_counts.index, dominant_counts.values)\n",
    "plt.title('Dominant Component in Blends')\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Number of Blends')\n",
    "\n",
    "# Add percentage labels\n",
    "for bar, pct in zip(bars, dominant_pct):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "             f'{pct:.1f}%', ha='center', va='bottom')\n",
    "plt.show()\n",
    "\n",
    "# Print the actual counts and percentages\n",
    "for comp, count in dominant_counts.items():\n",
    "    pct = count / len(train_data) * 100\n",
    "    print(f\"{comp}: {count} blends ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Component Properties Analysis\n",
    "\n",
    "Now let's analyze the properties of each component and their distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group properties by component\n",
    "component_properties = {}\n",
    "for i in range(1, 6):\n",
    "    component_props = [col for col in property_cols if f'Component{i}_' in col]\n",
    "    component_properties[f'Component{i}'] = component_props\n",
    "    print(f\"Component {i} properties: {len(component_props)} columns\")\n",
    "    print(f\"Example columns: {component_props[:3]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics for component properties\n",
    "for component, props in component_properties.items():\n",
    "    print(f\"\\n{component} Properties Summary Statistics:\")\n",
    "    display(train_data[props].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Component Properties\n",
    "\n",
    "Let's visualize the distribution of each property across components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot property distributions across components\n",
    "def plot_property_distributions(property_num):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        prop_col = f\"Component{i}_Property{property_num}\"\n",
    "        sns.kdeplot(train_data[prop_col], label=f\"Component {i}\")\n",
    "    \n",
    "    plt.title(f'Distribution of Property {property_num} Across Components')\n",
    "    plt.xlabel('Property Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Plot distributions for the first 3 properties\n",
    "for prop_num in range(1, 4):\n",
    "    plot_property_distributions(prop_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for all properties of Component 1\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.boxplot(data=train_data[component_properties['Component1']])\n",
    "plt.title('Component 1 Properties Box Plot')\n",
    "plt.ylabel('Property Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis Within Components\n",
    "\n",
    "Let's examine correlations between properties within each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for Component 1 properties\n",
    "plt.figure(figsize=(12, 10))\n",
    "comp1_corr = train_data[component_properties['Component1']].corr()\n",
    "sns.heatmap(comp1_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
    "plt.title('Correlation Between Component 1 Properties')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify highly correlated properties within a component\n",
    "def find_high_correlations(component_num, threshold=0.7):\n",
    "    comp_props = component_properties[f'Component{component_num}']\n",
    "    corr_matrix = train_data[comp_props].corr().abs()\n",
    "    \n",
    "    # Create a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix), k=1)\n",
    "    high_corr = corr_matrix.where(mask & (corr_matrix > threshold)).stack().reset_index()\n",
    "    high_corr.columns = ['Property1', 'Property2', 'Correlation']\n",
    "    high_corr = high_corr.sort_values('Correlation', ascending=False)\n",
    "    \n",
    "    return high_corr\n",
    "\n",
    "# Find high correlations for each component\n",
    "for i in range(1, 6):\n",
    "    high_corr = find_high_correlations(i)\n",
    "    if len(high_corr) > 0:\n",
    "        print(f\"\\nHighly correlated properties in Component {i}:\")\n",
    "        display(high_corr)\n",
    "    else:\n",
    "        print(f\"\\nNo highly correlated properties found in Component {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Component Property Analysis\n",
    "\n",
    "Let's analyze how the same property varies across different components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the same property across all components\n",
    "def compare_property_across_components(property_num):\n",
    "    prop_cols = [f\"Component{i}_Property{property_num}\" for i in range(1, 6)]\n",
    "    prop_df = train_data[prop_cols]\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    prop_df.columns = [f\"Component{i}\" for i in range(1, 6)]\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = prop_df.corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
    "    plt.title(f'Correlation of Property {property_num} Across Components')\n",
    "    plt.show()\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "# Compare the first 3 properties across components\n",
    "for prop_num in range(1, 4):\n",
    "    print(f\"\\nProperty {prop_num} cross-component correlation:\")\n",
    "    compare_property_across_components(prop_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property Variability Analysis\n",
    "\n",
    "Let's identify which properties have the highest and lowest variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coefficient of variation for all properties\n",
    "cv_data = {}\n",
    "for component, props in component_properties.items():\n",
    "    for prop in props:\n",
    "        mean = train_data[prop].mean()\n",
    "        std = train_data[prop].std()\n",
    "        # Avoid division by zero\n",
    "        if mean != 0:\n",
    "            cv = abs(std / mean)\n",
    "        else:\n",
    "            cv = float('inf')\n",
    "        cv_data[prop] = {'mean': mean, 'std': std, 'cv': cv}\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "cv_df = pd.DataFrame.from_dict(cv_data, orient='index')\n",
    "cv_df = cv_df.sort_values('cv', ascending=False)\n",
    "\n",
    "# Display properties with highest variability\n",
    "print(\"Properties with highest variability:\")\n",
    "display(cv_df.head(10))\n",
    "\n",
    "# Display properties with lowest variability\n",
    "print(\"\\nProperties with lowest variability:\")\n",
    "display(cv_df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize property variability by component\n",
    "component_cv = {}\n",
    "for component, props in component_properties.items():\n",
    "    component_cv[component] = cv_df.loc[props, 'cv'].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(component_cv.keys(), component_cv.values())\n",
    "plt.title('Average Property Variability by Component')\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Average Coefficient of Variation')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.2f}', ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Target Blend Properties Analysis\n",
    "\n",
    "Now let's analyze the target blend properties that we need to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target blend properties\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i, col in enumerate(target_cols, 1):\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.histplot(train_data[col], kde=True, bins=30)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel('Property Value')\n",
    "    plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots of target properties\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(data=train_data[target_cols])\n",
    "plt.title('Target Blend Properties Box Plot')\n",
    "plt.ylabel('Property Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis of Target Properties\n",
    "\n",
    "Let's examine correlations between the target blend properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for target properties\n",
    "plt.figure(figsize=(12, 10))\n",
    "target_corr = train_data[target_cols].corr()\n",
    "sns.heatmap(target_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
    "plt.title('Correlation Between Target Blend Properties')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated target properties\n",
    "print(\"\\nHighly correlated target properties (|r| > 0.7):\")\n",
    "high_corr_mask = (target_corr.abs() > 0.7) & (target_corr.abs() < 1.0)\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(target_corr.columns)):\n",
    "    for j in range(i+1, len(target_corr.columns)):\n",
    "        if high_corr_mask.iloc[i, j]:\n",
    "            prop1 = target_corr.columns[i]\n",
    "            prop2 = target_corr.columns[j]\n",
    "            corr_val = target_corr.iloc[i, j]\n",
    "            high_corr_pairs.append((prop1, prop2, corr_val))\n",
    "\n",
    "for prop1, prop2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "    print(f\"{prop1} and {prop2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot of target properties\n",
    "sns.pairplot(train_data[target_cols], diag_kind='kde', height=2)\n",
    "plt.suptitle('Pairwise Relationships Between Target Blend Properties', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship Between Component Fractions and Target Properties\n",
    "\n",
    "Let's analyze how component fractions relate to target blend properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between component fractions and target properties\n",
    "fraction_target_corr = train_data[fraction_cols + target_cols].corr().iloc[:len(fraction_cols), len(fraction_cols):]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(fraction_target_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
    "plt.title('Correlation Between Component Fractions and Target Properties')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Correlation between component fractions and target properties:\")\n",
    "fraction_target_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot relationship between a component fraction and target property\n",
    "def plot_fraction_target_relationship(fraction_col, target_col):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=fraction_col, y=target_col, data=train_data)\n",
    "    \n",
    "    # Add regression line\n",
    "    sns.regplot(x=fraction_col, y=target_col, data=train_data, scatter=False, color='red')\n",
    "    \n",
    "    plt.title(f'Relationship Between {fraction_col} and {target_col}')\n",
    "    plt.xlabel(fraction_col)\n",
    "    plt.ylabel(target_col)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Find the strongest correlations between fractions and targets\n",
    "strongest_corrs = []\n",
    "for fraction_col in fraction_cols:\n",
    "    for target_col in target_cols:\n",
    "        corr = fraction_target_corr.loc[fraction_col, target_col]\n",
    "        strongest_corrs.append((fraction_col, target_col, corr))\n",
    "\n",
    "# Sort by absolute correlation value\n",
    "strongest_corrs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "# Plot the top 3 strongest relationships\n",
    "print(\"Top 3 strongest relationships between component fractions and target properties:\")\n",
    "for i, (fraction_col, target_col, corr) in enumerate(strongest_corrs[:3]):\n",
    "    print(f\"{i+1}. {fraction_col} vs {target_col}: correlation = {corr:.3f}\")\n",
    "    plot_fraction_target_relationship(fraction_col, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship Between Component Properties and Target Properties\n",
    "\n",
    "Let's analyze how individual component properties relate to target blend properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find strongest correlations between component properties and target properties\n",
    "def find_top_property_correlations(n=10):\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = train_data[property_cols + target_cols].corr()\n",
    "    \n",
    "    # Extract correlations between component properties and target properties\n",
    "    prop_target_corr = corr_matrix.loc[property_cols, target_cols]\n",
    "    \n",
    "    # Convert to long format for easier sorting\n",
    "    corr_data = []\n",
    "    for prop_col in property_cols:\n",
    "        for target_col in target_cols:\n",
    "            corr_data.append((prop_col, target_col, prop_target_corr.loc[prop_col, target_col]))\n",
    "    \n",
    "    # Sort by absolute correlation value\n",
    "    corr_data.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    \n",
    "    return corr_data[:n]\n",
    "\n",
    "# Find top correlations\n",
    "top_correlations = find_top_property_correlations(15)\n",
    "\n",
    "# Display results\n",
    "print(\"Top 15 strongest correlations between component properties and target properties:\")\n",
    "for i, (prop_col, target_col, corr) in enumerate(top_correlations):\n",
    "    print(f\"{i+1}. {prop_col} vs {target_col}: correlation = {corr:.3f}\")\n",
    "\n",
    "# Create a DataFrame for easier visualization\n",
    "top_corr_df = pd.DataFrame(top_correlations, columns=['Component Property', 'Target Property', 'Correlation'])\n",
    "top_corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 3 strongest relationships\n",
    "for i, (prop_col, target_col, corr) in enumerate(top_correlations[:3]):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=prop_col, y=target_col, data=train_data)\n",
    "    \n",
    "    # Add regression line\n",
    "    sns.regplot(x=prop_col, y=target_col, data=train_data, scatter=False, color='red')\n",
    "    \n",
    "    plt.title(f'Relationship Between {prop_col} and {target_col}\\nCorrelation: {corr:.3f}')\n",
    "    plt.xlabel(prop_col)\n",
    "    plt.ylabel(target_col)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Component Properties Analysis\n",
    "\n",
    "Let's create weighted component properties based on their fractions and analyze their relationship with target properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted properties for each component\n",
    "weighted_props = {}\n",
    "\n",
    "for prop_num in range(1, 11):  # For each property type (1-10)\n",
    "    weighted_prop_name = f\"Weighted_Property{prop_num}\"\n",
    "    \n",
    "    # Initialize with zeros\n",
    "    train_data[weighted_prop_name] = 0\n",
    "    \n",
    "    # Add weighted contribution from each component\n",
    "    for comp_num in range(1, 6):  # For each component (1-5)\n",
    "        fraction_col = f\"Component{comp_num}_fraction\"\n",
    "        prop_col = f\"Component{comp_num}_Property{prop_num}\"\n",
    "        \n",
    "        train_data[weighted_prop_name] += train_data[fraction_col] * train_data[prop_col]\n",
    "    \n",
    "    weighted_props[weighted_prop_name] = weighted_prop_name\n",
    "\n",
    "weighted_prop_cols = list(weighted_props.values())\n",
    "print(f\"Created {len(weighted_prop_cols)} weighted property columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between weighted properties and target properties\n",
    "weighted_target_corr = train_data[weighted_prop_cols + target_cols].corr().iloc[:len(weighted_prop_cols), len(weighted_prop_cols):]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(weighted_target_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
    "plt.title('Correlation Between Weighted Component Properties and Target Properties')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Correlation between weighted properties and target properties:\")\n",
    "weighted_target_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the strongest relationships between weighted properties and targets\n",
    "strongest_weighted_corrs = []\n",
    "for weighted_col in weighted_prop_cols:\n",
    "    for target_col in target_cols:\n",
    "        corr = weighted_target_corr.loc[weighted_col, target_col]\n",
    "        strongest_weighted_corrs.append((weighted_col, target_col, corr))\n",
    "\n",
    "# Sort by absolute correlation value\n",
    "strongest_weighted_corrs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "# Plot the top 3 strongest relationships\n",
    "print(\"Top 3 strongest relationships between weighted properties and target properties:\")\n",
    "for i, (weighted_col, target_col, corr) in enumerate(strongest_weighted_corrs[:3]):\n",
    "    print(f\"{i+1}. {weighted_col} vs {target_col}: correlation = {corr:.3f}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=weighted_col, y=target_col, data=train_data)\n",
    "    \n",
    "    # Add regression line\n",
    "    sns.regplot(x=weighted_col, y=target_col, data=train_data, scatter=False, color='red')\n",
    "    \n",
    "    plt.title(f'Relationship Between {weighted_col} and {target_col}\\nCorrelation: {corr:.3f}')\n",
    "    plt.xlabel(weighted_col)\n",
    "    plt.ylabel(target_col)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
},
  
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Feature Engineering Insights\n",
    "\n",
    "Based on our exploratory data analysis, let's identify potential feature engineering strategies that could improve our predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Weighted Component Properties\n",
    "\n",
    "We've already seen that weighted properties (component property × component fraction) show strong correlations with target properties. Let's evaluate how much better these weighted features perform compared to raw properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare correlations: raw properties vs weighted properties\n",
    "comparison_data = []\n",
    "\n",
    "for prop_num in range(1, 11):\n",
    "    weighted_prop = f\"Weighted_Property{prop_num}\"\n",
    "    \n",
    "    for target_col in target_cols:\n",
    "        # Get correlation for weighted property\n",
    "        weighted_corr = train_data[[weighted_prop, target_col]].corr().iloc[0, 1]\n",
    "        \n",
    "        # Get max correlation among raw properties of this type\n",
    "        raw_corrs = []\n",
    "        for comp_num in range(1, 6):\n",
    "            raw_prop = f\"Component{comp_num}_Property{prop_num}\"\n",
    "            raw_corr = train_data[[raw_prop, target_col]].corr().iloc[0, 1]\n",
    "            raw_corrs.append((raw_prop, raw_corr))\n",
    "        \n",
    "        # Find max correlation among raw properties\n",
    "        max_raw_prop, max_raw_corr = max(raw_corrs, key=lambda x: abs(x[1]))\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Property_Type': f\"Property{prop_num}\",\n",
    "            'Target': target_col,\n",
    "            'Weighted_Correlation': weighted_corr,\n",
    "            'Best_Raw_Property': max_raw_prop,\n",
    "            'Raw_Correlation': max_raw_corr,\n",
    "            'Improvement': abs(weighted_corr) - abs(max_raw_corr)\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and sort by improvement\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Improvement', ascending=False)\n",
    "\n",
    "# Display top improvements\n",
    "print(\"Top cases where weighted properties outperform raw properties:\")\n",
    "display(comparison_df.head(10))\n",
    "\n",
    "# Display cases where raw properties perform better\n",
    "print(\"\\nCases where raw properties outperform weighted properties:\")\n",
    "display(comparison_df[comparison_df['Improvement'] < 0].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Component Interaction Features\n",
    "\n",
    "Let's create interaction features between component fractions to capture potential synergistic effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairwise interaction features between component fractions\n",
    "interaction_features = {}\n",
    "\n",
    "for i in range(len(fraction_cols)):\n",
    "    for j in range(i+1, len(fraction_cols)):\n",
    "        col1 = fraction_cols[i]\n",
    "        col2 = fraction_cols[j]\n",
    "        interaction_name = f\"{col1}_{col2}_interaction\"\n",
    "        \n",
    "        # Create interaction feature\n",
    "        train_data[interaction_name] = train_data[col1] * train_data[col2]\n",
    "        interaction_features[interaction_name] = interaction_name\n",
    "\n",
    "interaction_cols = list(interaction_features.values())\n",
    "print(f\"Created {len(interaction_cols)} interaction features\")\n",
    "\n",
    "# Calculate correlations with target properties\n",
    "interaction_target_corr = train_data[interaction_cols + target_cols].corr().iloc[:len(interaction_cols), len(interaction_cols):]\n",
    "\n",
    "# Find strongest correlations\n",
    "strongest_interactions = []\n",
    "for interaction_col in interaction_cols:\n",
    "    for target_col in target_cols:\n",
    "        corr = interaction_target_corr.loc[interaction_col, target_col]\n",
    "        strongest_interactions.append((interaction_col, target_col, corr))\n",
    "\n",
    "# Sort by absolute correlation value\n",
    "strongest_interactions.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "# Display top correlations\n",
    "print(\"\\nTop 10 strongest correlations between interaction features and target properties:\")\n",
    "for i, (interaction_col, target_col, corr) in enumerate(strongest_interactions[:10]):\n",
    "    print(f\"{i+1}. {interaction_col} vs {target_col}: correlation = {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Polynomial Features for Component Fractions\n",
    "\n",
    "Let's explore if squared terms of component fractions might capture non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create squared features for component fractions\n",
    "squared_features = {}\n",
    "\n",
    "for col in fraction_cols:\n",
    "    squared_name = f\"{col}_squared\"\n",
    "    train_data[squared_name] = train_data[col] ** 2\n",
    "    squared_features[squared_name] = squared_name\n",
    "\n",
    "squared_cols = list(squared_features.values())\n",
    "\n",
    "# Calculate correlations with target properties\n",
    "squared_target_corr = train_data[squared_cols + target_cols].corr().iloc[:len(squared_cols), len(squared_cols):]\n",
    "\n",
    "# Find strongest correlations\n",
    "strongest_squared = []\n",
    "for squared_col in squared_cols:\n",
    "    for target_col in target_cols:\n",
    "        corr = squared_target_corr.loc[squared_col, target_col]\n",
    "        strongest_squared.append((squared_col, target_col, corr))\n",
    "\n",
    "# Sort by absolute correlation value\n",
    "strongest_squared.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "# Display top correlations\n",
    "print(\"Top 5 strongest correlations between squared features and target properties:\")\n",
    "for i, (squared_col, target_col, corr) in enumerate(strongest_squared[:5]):\n",
    "    print(f\"{i+1}. {squared_col} vs {target_col}: correlation = {corr:.3f}\")\n",
    "    \n",
    "    # Compare with linear correlation\n",
    "    original_col = squared_col.replace('_squared', '')\n",
    "    linear_corr = train_data[[original_col, target_col]].corr().iloc[0, 1]\n",
    "    print(f\"   Original linear correlation ({original_col}): {linear_corr:.3f}\")\n",
    "    print(f\"   Improvement: {abs(corr) - abs(linear_corr):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Component Property Ratios\n",
    "\n",
    "Let's explore if ratios between different properties of the same component might be informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ratio features for selected property pairs within each component\n",
    "# We'll focus on properties that showed high correlations with targets\n",
    "ratio_features = {}\n",
    "\n",
    "# Define some property pairs to create ratios for\n",
    "property_pairs = [(1, 2), (1, 3), (2, 3), (4, 5), (6, 7), (8, 9)]\n",
    "\n",
    "for comp_num in range(1, 6):\n",
    "    for prop1, prop2 in property_pairs:\n",
    "        prop1_col = f\"Component{comp_num}_Property{prop1}\"\n",
    "        prop2_col = f\"Component{comp_num}_Property{prop2}\"\n",
    "        ratio_name = f\"Component{comp_num}_Ratio_{prop1}_{prop2}\"\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        denominator = train_data[prop2_col].copy()\n",
    "        denominator = denominator.replace(0, np.nan)  # Replace zeros with NaN\n",
    "        \n",
    "        # Create ratio feature\n",
    "        train_data[ratio_name] = train_data[prop1_col] / denominator\n",
    "        train_data[ratio_name] = train_data[ratio_name].fillna(0)  # Replace NaN with 0\n",
    "        \n",
    "        ratio_features[ratio_name] = ratio_name\n",
    "\n",
    "ratio_cols = list(ratio_features.values())\n",
    "print(f\"Created {len(ratio_cols)} ratio features\")\n",
    "\n",
    "# Calculate correlations with target properties\n",
    "ratio_target_corr = train_data[ratio_cols + target_cols].corr().iloc[:len(ratio_cols), len(ratio_cols):]\n",
    "\n",
    "# Find strongest correlations\n",
    "strongest_ratios = []\n",
    "for ratio_col in ratio_cols:\n",
    "    for target_col in target_cols:\n",
    "        corr = ratio_target_corr.loc[ratio_col, target_col]\n",
    "        strongest_ratios.append((ratio_col, target_col, corr))\n",
    "\n",
    "# Sort by absolute correlation value\n",
    "strongest_ratios.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "# Display top correlations\n",
    "print(\"\\nTop 10 strongest correlations between ratio features and target properties:\")\n",
    "for i, (ratio_col, target_col, corr) in enumerate(strongest_ratios[:10]):\n",
    "    print(f\"{i+1}. {ratio_col} vs {target_col}: correlation = {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Dominant Component Indicators\n",
    "\n",
    "Let's create indicator features for the dominant component in each blend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encoded features for dominant component\n",
    "dominant_indicators = pd.get_dummies(train_data['dominant_component'], prefix='dominant')\n",
    "train_data = pd.concat([train_data, dominant_indicators], axis=1)\n",
    "\n",
    "dominant_cols = dominant_indicators.columns.tolist()\n",
    "print(f\"Created {len(dominant_cols)} dominant component indicator features\")\n",
    "\n",
    "# Calculate correlations with target properties\n",
    "dominant_target_corr = train_data[dominant_cols + target_cols].corr().iloc[:len(dominant_cols), len(dominant_cols):]\n",
    "\n",
    "# Display correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(dominant_target_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
    "plt.title('Correlation Between Dominant Component Indicators and Target Properties')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Summary and Modeling Recommendations\n",
    "\n",
    "Based on our exploratory data analysis, here are the key insights and recommendations for modeling the fuel blend properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights from EDA\n",
    "\n",
    "1. **Component Usage Patterns**:\n",
    "   - Some components are used more frequently than others in blends\n",
    "   - Component fractions show significant variability\n",
    "   - Certain components tend to dominate in blends\n",
    "\n",
    "2. **Property Relationships**:\n",
    "   - Component properties show varying degrees of correlation with target properties\n",
    "   - Some target properties are highly correlated with each other\n",
    "   - Weighted properties often show stronger correlations with targets than raw properties\n",
    "\n",
    "3. **Non-linear Relationships**:\n",
    "   - Evidence of non-linear relationships between inputs and outputs\n",
    "   - Interaction effects between components are likely important\n",
    "   - Polynomial features show improved correlations in some cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Recommendations\n",
    "\n",
    "Based on our analysis, we recommend the following feature engineering strategies:\n",
    "\n",
    "1. **Weighted Component Properties**:\n",
    "   - Create weighted versions of all component properties (property × fraction)\n",
    "   - These capture the proportional contribution of each component\n",
    "\n",
    "2. **Interaction Features**:\n",
    "   - Generate pairwise interactions between component fractions\n",
    "   - Create interactions between key component properties\n",
    "\n",
    "3. **Polynomial Features**:\n",
    "   - Include squared terms for component fractions\n",
    "   - Consider higher-order polynomials for properties showing strong non-linear relationships\n",
    "\n",
    "4. **Ratio Features**:\n",
    "   - Create ratios between related properties within components\n",
    "   - These can capture important property relationships\n",
    "\n",
    "5. **Dominant Component Indicators**:\n",
    "   - Include indicators for the dominant component in each blend\n",
    "   - These can help capture threshold effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Strategy Recommendations\n",
    "\n",
    "1. **Model Selection**:\n",
    "   - Use algorithms capable of capturing non-linear relationships (Random Forest, Gradient Boosting, Neural Networks)\n",
    "   - Consider ensemble methods to combine different modeling approaches\n",
    "\n",
    "2. **Target-Specific Models**:\n",
    "   - Consider building separate models for each target property\n",
    "   - This allows for property-specific feature engineering\n",
    "\n",
    "3. **Validation Strategy**:\n",
    "   - Use k-fold cross-validation to ensure model robustness\n",
    "   - Optimize for MAPE (Mean Absolute Percentage Error) to match the competition metric\n",
    "\n",
    "4. **Feature Selection**:\n",
    "   - Start with all engineered features\n",
    "   - Use feature importance methods to identify and retain the most predictive features\n",
    "   - Consider dimensionality reduction for highly correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "1. Implement the recommended feature engineering strategies\n",
    "2. Build and evaluate baseline models\n",
    "3. Refine feature selection based on model performance\n",
    "4. Optimize hyperparameters for the best-performing models\n",
    "5. Create ensemble models to improve prediction accuracy\n",
    "6. Generate predictions for the test set"
   ]
  }
]